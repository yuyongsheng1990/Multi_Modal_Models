{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d56b7399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:20.934615Z",
     "start_time": "2024-07-17T02:11:20.918378Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPT generated cross-attention codes\n",
    "# cross-attention用于在不同模态之间(比如text and image)进行信息交互。\n",
    "# image embeddings as q to attention with text embeddings as k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44efb312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:22:04.093905Z",
     "start_time": "2024-07-16T07:22:04.090713Z"
    }
   },
   "source": [
    "## Cross_Attention: image to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f41e16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:27.485851Z",
     "start_time": "2024-07-17T02:11:21.991250Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a11b6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:27.492344Z",
     "start_time": "2024-07-17T02:11:27.488352Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeaef3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:36:09.619756Z",
     "start_time": "2024-07-16T07:36:09.616140Z"
    }
   },
   "source": [
    "### cross_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c2e27e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:27.506305Z",
     "start_time": "2024-07-17T02:11:27.495336Z"
    }
   },
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # q, k, v projection layers\n",
    "        self.q_proj = nn.Linear(d_model, d_model)  # (2048, 2048)\n",
    "        self.k_proj = nn.Linear(768, d_model)  \n",
    "        self.v_proj = nn.Linear(768, d_model)\n",
    "        \n",
    "        # output projection layer\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Scaling factor\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model // num_heads])).to(device)\n",
    "    \n",
    "    def forward(self, queries, keys, values, mask=None):  # image, text, text\n",
    "        batch_size = queries.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        queries = self.q_proj(queries)  # (batch_size, seq_len, d_model)\n",
    "        keys = self.k_proj(keys)\n",
    "        values = self.v_proj(values)\n",
    "        \n",
    "        # Split into multiple heads and transpose\n",
    "        queries = queries.view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).transpose(1, 2)\n",
    "        keys = keys.view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).transpose(1, 2)\n",
    "        values = values.view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "        \n",
    "        # Concatenate multiple heads and put through final linear layer\n",
    "        attention_output = attention_output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)  # contiguous表示连续存储。\n",
    "        output = self.out_proj(attention_output)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c924f38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:35:59.822272Z",
     "start_time": "2024-07-16T07:35:59.819343Z"
    }
   },
   "source": [
    "### multi-modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e417a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:27.514627Z",
     "start_time": "2024-07-17T02:11:27.508324Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, image_model, text_model, d_model, num_heads):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        self.image_model = image_model\n",
    "        self.text_model = text_model\n",
    "        self.cross_attention = CrossAttention(d_model, num_heads)\n",
    "        self.fc = nn.Linear(d_model, 1)   # 假设二分类任务\n",
    "        \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # image embeddings\n",
    "        image_features = self.image_model(images)\n",
    "        print('image_feature_shape:{}'.format(image_features.shape))\n",
    "        \n",
    "        # text embeddings\n",
    "        text_outputs = self.text_model(input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state\n",
    "        print('text_feature_shape:{}'.format(text_features.shape))\n",
    "        \n",
    "        '''\n",
    "        transformers.BertModel默认输出是BaseModelOutputWithPoolingAndCrossAttention，这是一个包含多个字段命名元组NamedTuple。\n",
    "        last_hidden_state是bert_model最后一层的隐藏状态, 是一个shape=(batch_size, seq_len, hidden_size)的tensor。\n",
    "        NamedTuple还包括的参数：\n",
    "            - pooler_output，表示池化后的输出，这个输出可以用于分类任务。\n",
    "            - attention_weights,\n",
    "        '''\n",
    "        \n",
    "        # cross attention\n",
    "        cross_output, attention_weights = self.cross_attention(image_features.unsqueeze(1), text_features, text_features)\n",
    "        \n",
    "        # classfication\n",
    "        output = self.fc(cross_output.squeeze(1))\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16bc633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T00:39:33.400564Z",
     "start_time": "2024-07-17T00:39:33.397432Z"
    }
   },
   "source": [
    "### image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ce60cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:27.523576Z",
     "start_time": "2024-07-17T02:11:27.517601Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageModel, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # 移除ResNet模型的最后一层全连接层\n",
    "        '''\n",
    "        ResNet模型的最后一层通常是一个全连接层，用于分类任务。对于pretrained ResNet-50模型，这个全连接层的的作用是讲ResNet的output features\n",
    "        (通常是一个2048维的向量)映射到一个指定数量的类别上。例如，Rest在ImageNet数据集上预训练，最后一层全连接层输出维度是1000，对应于ImageNet\n",
    "        的1000个类别。在多模态学习中，我们通常不需要最后一层的分类器，而是需要获取iamge embeddings，以便与其他模态(i.e. text)特征进行融合。\n",
    "        这种情况下，我们需要移除最后一层全连接层，只保留ResNet的特征提取部分。\n",
    "        nn.Identity作用是torch的一个占位符层，它不改变输入的值，只是简单地返回输入。使用nn.Identity可以方便地移除某一层而不改变模型地其他部分!!!\n",
    "        '''\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7017b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:01:19.682267Z",
     "start_time": "2024-07-17T01:01:19.678684Z"
    }
   },
   "source": [
    "### text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c3bce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:28.541540Z",
     "start_time": "2024-07-17T02:11:27.525569Z"
    }
   },
   "outputs": [],
   "source": [
    "text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a27e75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:05:48.759506Z",
     "start_time": "2024-07-17T01:05:48.756893Z"
    }
   },
   "source": [
    "### data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4458a171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:28.547445Z",
     "start_time": "2024-07-17T02:11:28.543532Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e0a83c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:28.570772Z",
     "start_time": "2024-07-17T02:11:28.551429Z"
    }
   },
   "outputs": [],
   "source": [
    "# data input\n",
    "image = Image.open('./data/five.jpg')\n",
    "text = 'five is a super pretty girl.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "210c5e13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:28.578696Z",
     "start_time": "2024-07-17T02:11:28.573757Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocess\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7eb6571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:28.611931Z",
     "start_time": "2024-07-17T02:11:28.580676Z"
    }
   },
   "outputs": [],
   "source": [
    "image = image_transforms(image).unsqueeze(0)\n",
    "# tokenizer分词对象，来自transformers.BertTokenizer或RobertaTokenizer; truncation=True对长度超过max_length的文本进行截断；\n",
    "# padding填充到最大长度。\n",
    "text_tokens = tokenizer(text, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "input_ids = text_tokens['input_ids']\n",
    "attention_mask = text_tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aeab51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:38:21.413418Z",
     "start_time": "2024-07-17T01:38:21.409141Z"
    }
   },
   "source": [
    "### running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47fecd59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:29.164087Z",
     "start_time": "2024-07-17T02:11:28.613923Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# configs\n",
    "d_model = 2048\n",
    "num_heads = 8\n",
    "\n",
    "# initialize model\n",
    "image_model = ImageModel()\n",
    "multi_modal_model = MultiModalModel(image_model, text_model, d_model, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeeb013f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:29.174933Z",
     "start_time": "2024-07-17T02:11:29.166445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07b2c384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:29.181429Z",
     "start_time": "2024-07-17T02:11:29.176921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "affb94a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:11:29.570068Z",
     "start_time": "2024-07-17T02:11:29.183420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_feature_shape:torch.Size([1, 2048])\n",
      "text_feature_shape:torch.Size([1, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "# feedforward\n",
    "output, attention_weights = multi_modal_model(image, input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "644b58d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:12:01.551283Z",
     "start_time": "2024-07-17T02:12:01.545391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0751]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17acb7bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T02:12:14.435256Z",
     "start_time": "2024-07-17T02:12:14.430283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape  # cross_attention_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
