{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d5b4ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:18:27.017123Z",
     "start_time": "2024-07-16T00:18:27.012138Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPT-4 generated codes\n",
    "# torch实现一个结合image和text的多模态model。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337aad2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:18:41.972349Z",
     "start_time": "2024-07-16T00:18:41.969158Z"
    }
   },
   "source": [
    "## GPT-4 Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3442a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:22:19.791082Z",
     "start_time": "2024-07-16T00:22:18.867613Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb896188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:33:07.752946Z",
     "start_time": "2024-07-16T00:33:07.748956Z"
    }
   },
   "source": [
    "### model_construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01823c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:22:42.052635Z",
     "start_time": "2024-07-16T00:22:42.049316Z"
    }
   },
   "source": [
    "#### image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaeec1b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:26:25.628266Z",
     "start_time": "2024-07-16T00:26:25.623789Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageModel, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fn = nn.Identity()  # 移除ResNet模型的最后一层全连接层\n",
    "        '''\n",
    "        ResNet模型的最后一层通常是一个全连接层，用于分类任务。对于pretrained ResNet-50模型，这个全连接层的的作用是讲ResNet的output features\n",
    "        (通常是一个2048维的向量)映射到一个指定数量的类别上。例如，Rest在ImageNet数据集上预训练，最后一层全连接层输出维度是1000，对应于ImageNet\n",
    "        的1000个类别。在多模态学习中，我们通常不需要最后一层的分类器，而是需要获取iamge embeddings，以便与其他模态(i.e. text)特征进行融合。\n",
    "        这种情况下，我们需要移除最后一层全连接层，只保留ResNet的特征提取部分。\n",
    "        nn.Identity作用是torch的一个占位符层，它不改变输入的值，只是简单地返回输入。使用nn.Identity可以方便地移除某一层而不改变模型地其他部分!!!\n",
    "        '''\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395127b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:26:31.701045Z",
     "start_time": "2024-07-16T00:26:31.696766Z"
    }
   },
   "source": [
    "#### text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f47cd2d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:28:57.754986Z",
     "start_time": "2024-07-16T00:28:57.750000Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output  # 使用bert的pooling output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22219e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:29:12.138966Z",
     "start_time": "2024-07-16T00:29:12.135450Z"
    }
   },
   "source": [
    "#### multi-modal integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58a86f1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T02:11:22.140913Z",
     "start_time": "2024-07-16T02:11:22.135418Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        self.image_model = ImageModel()\n",
    "        self.text_model = TextModel()\n",
    "        self.fc = nn.Linear(1768, 1)  # 假设是二分类任务\n",
    "    \n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.image_model(image)\n",
    "        print('image_features_shape: {}'.format(image_features.shape))\n",
    "        text_features = self.text_model(input_ids, attention_mask)\n",
    "        print('text_features_shape: {}'.format(text_features.shape))\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        print('combined_features_shape: {}'.format(combined_features.shape))\n",
    "        output = self.fc(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebbc072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:34:10.892155Z",
     "start_time": "2024-07-16T00:34:10.888525Z"
    }
   },
   "source": [
    "### data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e23e782",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:34:08.448150Z",
     "start_time": "2024-07-17T01:34:08.443702Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33dd82af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:34:09.539262Z",
     "start_time": "2024-07-17T01:34:09.169066Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f3b75ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:34:11.153578Z",
     "start_time": "2024-07-17T01:34:11.149737Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(image, text):\n",
    "    image = image_transforms(image)\n",
    "    text_tokens = tokenizer(text, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "    return image, text_tokens['input_ids'].squeeze(), text_tokens['attention_mask'].squeeze(),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b52ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:38:55.983100Z",
     "start_time": "2024-07-16T00:38:55.979824Z"
    }
   },
   "source": [
    "### running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b84d0c62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:34:11.975270Z",
     "start_time": "2024-07-17T01:34:11.972280Z"
    }
   },
   "outputs": [],
   "source": [
    "# image data\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c1d3687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:34:13.254682Z",
     "start_time": "2024-07-17T01:34:13.250592Z"
    }
   },
   "outputs": [],
   "source": [
    "image = Image.open('./data/five.jpg')\n",
    "text = 'five is a super pretty girl.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8d74b5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:34:14.675347Z",
     "start_time": "2024-07-17T01:34:14.640584Z"
    }
   },
   "outputs": [],
   "source": [
    "image, input_ids, attention_mask = preprocess_data(image, text)\n",
    "image = image.unsqueeze(0)  # 扩展batch dimension\n",
    "input_ids = input_ids.unsqueeze(0)\n",
    "attention_mask = attention_mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e5e8fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T02:11:28.588414Z",
     "start_time": "2024-07-16T02:11:27.454039Z"
    }
   },
   "outputs": [],
   "source": [
    "# initilize model\n",
    "model = MultiModalModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ac49b2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T00:54:39.571137Z",
     "start_time": "2024-07-16T00:54:39.565723Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义loss和optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "019071e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T02:11:30.302413Z",
     "start_time": "2024-07-16T02:11:29.928977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_shape: torch.Size([1, 1000])\n",
      "text_features_shape: torch.Size([1, 768])\n",
      "combined_features_shape: torch.Size([1, 1768])\n",
      "tensor([[-0.1499]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# feed-forward\n",
    "output = model(image, input_ids, attention_mask)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dc4bd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T02:12:53.282827Z",
     "start_time": "2024-07-16T02:12:53.272473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39f502d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T02:14:36.799059Z",
     "start_time": "2024-07-16T02:14:36.788782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7709, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# label\n",
    "label = torch.tensor([1.0])\n",
    "\n",
    "loss = criterion(output.squeeze(dim=-1), label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf397820",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T02:15:11.253916Z",
     "start_time": "2024-07-16T02:15:10.311272Z"
    }
   },
   "outputs": [],
   "source": [
    "# BP\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd7ced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T02:15:26.187837Z",
     "start_time": "2024-07-16T02:15:26.184751Z"
    }
   },
   "source": [
    "### save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fb0eda8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T02:16:31.680557Z",
     "start_time": "2024-07-16T02:16:31.019334Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'./outputs/multi_modal_01.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
